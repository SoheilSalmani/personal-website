---
title: Airflow
relatedResources:
  - "9781617295508"
tags:
  - Airflow
  - Python
  - Data Engineering
---

1. Inspect this [endpoint](https://ll.thespacedevs.com/2.0.0/launch/upcoming)
   using cURL.

   <Solution>

   ```sh
   curl -L "https://ll.thespacedevs.com/2.0.0/launch/upcoming"
   ```

   </Solution>

2. Write an Airflow DAG to download images of rocket launches.

   <Solution>

   ```python fp=download_rocket_launches.py nu
   import json
   import pathlib

   import airflow
   import requests
   import requests.exceptions as requests_exceptions
   from airflow import DAG
   from airflow.operators.bash import BashOperator
   from airflow.operators.python import PythonOperator

   dag = DAG(
       dag_id="download_rocket_launches",
       description="Download rocket pictures of recently launched rockets.",
       start_date=airflow.utils.dates.days_ago(14),
       schedule_interval=None,
   )

   download_launches = BashOperator(
       task_id="download_launches",
       bash_command="curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'",  # noqa: E501
       dag=dag,
   )

   def _get_pictures():
       # Ensure directory exists
       pathlib.Path("/tmp/images").mkdir(parents=True, exist_ok=True)

       # Download all pictures in launches.json
       with open("/tmp/launches.json") as f:
           launches = json.load(f)
           image_urls = [launch["image"] for launch in launches["results"]]
           for image_url in image_urls:
               try:
                   response = requests.get(image_url)
                   image_filename = image_url.split("/")[-1]
                   target_file = f"/tmp/images/{image_filename}"
                   with open(target_file, "wb") as f:
                       f.write(response.content)
                   print(f"Downloaded {image_url} to {target_file}")
               except requests_exceptions.MissingSchema:
                   print(f"{image_url} appears to be an invalid URL.")
               except requests_exceptions.ConnectionError:
                   print(f"Could not connect to {image_url}.")

   get_pictures = PythonOperator(
       task_id="get_pictures", python_callable=_get_pictures, dag=dag
   )

   notify = BashOperator(
       task_id="notify",
       bash_command='echo "There are now $(ls /tmp/images/ | wc -l) images."',
       dag=dag,
   )

   download_launches >> get_pictures >> notify
   ```

   </Solution>

3. Configure the DAG to run once a day at midnight

   <Solution>

   ```python fp=download_rocket_launches.py hl=6
   ...

   dag = DAG(
       dag_id="download_rocket_launches",
       start_date=airflow.utils.dates.days_ago(14),
       schedule_interval="@daily",
   )

   ...
   ```

   </Solution>

4. Build the Events API.

   <Solution>

   ```dockerfile fp=Dockerfile nu
   FROM python:3.9-slim

   COPY requirements.txt /tmp/requirements.txt
   RUN pip install -r /tmp/requirements.txt && rm -f /tmp/requirements.txt

   COPY app.py /

   EXPOSE 5000

   ENTRYPOINT ["python"]
   CMD ["/app.py"]
   ```

   ```python fp=app.py nu
   from datetime import date, datetime, timedelta
   import time

   from numpy import random
   import pandas as pd
   from faker import Faker
   from flask import Flask, jsonify, request

   def _generate_events(end_date):
       events = pd.concat(
           [
               _generate_events_for_day(date=end_date - timedelta(days=(30 - i)))
               for i in range(30)
           ],
           axis=0,
       )
       return events

   def _generate_events_for_day(date):
       seed = int(time.mktime(date.timetuple()))
       Faker.seed(seed)
       random_state = random.RandomState(seed)

       n_users = random_state.randint(low=50, high=100)
       n_events = random_state.randint(low=200, high=2000)

       fake = Faker()
       users = [fake.ipv4() for _ in range(n_users)]

       return pd.DataFrame(
           {
               "user": random_state.choice(users, size=n_events, replace=True),
               "date": pd.to_datetime(date),
           }
       )

   app = Flask(__name__)
   app.config["events"] = _generate_events(end_date=date(year=2019, month=1, day=5))

   @app.route("/events")
   def events():
       start_date = _str_to_datetime(request.args.get("start_date", None))
       end_date = _str_to_datetime(request.args.get("end_date", None))

       events = app.config.get("events")
       if start_date is not None:
           events = events.loc[events["date"] >= start_date]
       if end_date is not None:
           events = events.loc[events["date"] < end_date]

       return jsonify(events.to_dict(orient="records"))

   def _str_to_datetime(value):
       if value is None:
           return None
       return datetime.strptime(value, "%Y-%m-%d")

   if __name__ == "__main__":
       app.run(host="0.0.0.0", port=5000)
   ```

   ```txt fp=requirements.txt nu
   click==8.0.3
   faker==11.3.0
   flask==2.0.2
   pandas==1.4.0
   ```

   </Solution>

5. Build a DAG which calculates stats in a CSV file stored locally.

   <Solution>

   ```python fp=calculate_event_stats.py nu
   import datetime as dt
   from pathlib import Path

   import pandas as pd
   from airflow import DAG
   from airflow.operators.bash import BashOperator
   from airflow.operators.python import PythonOperator

   dag = DAG(
       dag_id="fetch_events",
       start_date=dt.datetime(2019, 1, 1),
       schedule_interval=None,
   )

   fetch_events = BashOperator(
       task_id="fetch_events",
       bash_command=(
           "mkdir -p /home/airflow/data && "
           "curl -o /home/airflow/data/events.json http://events_api:5000/events"
       ),
       dag=dag,
   )

   def _calculate_stats(input_path, output_path):
       events = pd.read_json(input_path)
       stats = events.groupby(["date", "user"]).size().reset_index()
       Path(output_path).parent.mkdir(exist_ok=True)
       stats.to_csv(output_path, index=False)

   calculate_stats = PythonOperator(
       task_id="calculate_stats",
       python_callable=_calculate_stats,
       op_kwargs={
           "input_path": "/home/airflow/data/events.json",
           "output_path": "/home/airflow/data/stats.csv",
       },
       dag=dag,
   )

   fetch_events >> calculate_stats
   ```

   </Solution>
