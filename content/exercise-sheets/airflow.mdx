---
title: Airflow
relatedResources:
  - "9781617295508"
tags:
  - Airflow
  - Python
  - Data Engineering
---

1. Inspect this [endpoint](https://ll.thespacedevs.com/2.0.0/launch/upcoming)
   using cURL.

   <Solution>

   ```sh
   curl -L "https://ll.thespacedevs.com/2.0.0/launch/upcoming"
   ```

   </Solution>

2. Write an Airflow DAG to download images of rocket launches.

   <Solution>

   ```python fp=download_rocket_launches.py nu
   import json
   from pathlib import Path

   import airflow
   import requests
   import requests.exceptions as requests_exceptions
   from airflow import DAG
   from airflow.operators.bash import BashOperator
   from airflow.operators.python import PythonOperator

   AIRFLOW_HOME = "/home/airflow"

   dag = DAG(
       dag_id="download_rocket_launches",
       description="Download rocket pictures of recently launched rockets.",
       start_date=airflow.utils.dates.days_ago(14),
       schedule_interval=None,
   )

   download_launches = BashOperator(
       task_id="download_launches",
       bash_command=f"curl -L -o {AIRFLOW_HOME}/launches.json https://ll.thespacedevs.com/2.0.0/launch/upcoming",
       dag=dag,
   )

   def _get_images():
       # Ensure directory exists
       Path(f"{AIRFLOW_HOME}/images").mkdir(parents=True, exist_ok=True)

       # Download all pictures in launches.json
       with open(f"{AIRFLOW_HOME}/launches.json") as f:
           launches = json.load(f)
           image_urls = [launch["image"] for launch in launches["results"]]
           for image_url in image_urls:
               try:
                   response = requests.get(image_url)
                   image_filename = image_url.split("/")[-1]
                   target_file = f"{AIRFLOW_HOME}/images/{image_filename}"
                   with open(target_file, "wb") as f:
                       f.write(response.content)
               except requests_exceptions.MissingSchema:
                   print(f"{image_url} appears to be an invalid URL.")
               except requests_exceptions.ConnectionError:
                   print(f"Could not connect to {image_url}.")
               print(f"Downloaded {image_url} to {target_file}")

   get_images = PythonOperator(task_id="get_images", python_callable=_get_images, dag=dag)

   notify = BashOperator(
       task_id="notify",
       bash_command=f'echo "There are now $(ls {AIRFLOW_HOME}/images | wc -l) images."',
       dag=dag,
   )

   download_launches >> get_images >> notify
   ```

   </Solution>

3. Configure the DAG to run once a day at midnight

   <Solution>

   ```python fp=download_rocket_launches.py hl=6
   ...

   dag = DAG(
       dag_id="download_rocket_launches",
       start_date=airflow.utils.dates.days_ago(14),
       schedule_interval="@daily",
   )

   ...
   ```

   </Solution>

4. Build the Events API.

   <Solution>

   ```dockerfile fp=Dockerfile nu
   FROM python:3.9-slim

   WORKDIR /app

   COPY app.py .
   COPY requirements.txt .
   RUN pip install -r requirements.txt

   EXPOSE 5000

   ENTRYPOINT ["python"]
   CMD ["/app/app.py"]
   ```

   ```python fp=app.py nu
   import time
   from datetime import date, datetime, timedelta

   import pandas as pd
   from faker import Faker
   from flask import jsonify, Flask, request
   from numpy import random

   def _generate_events(end_date):
       events = pd.concat(
           [
               _generate_events_for_day(date=end_date - timedelta(days=(30 - i)))
               for i in range(30)
           ],
           axis=0,
       )
       return events

   def _generate_events_for_day(date):
       seed = int(time.mktime(date.timetuple()))
       generator = random.default_rng(seed=seed)
       Faker.seed(seed)

       n_users = generator.integers(low=50, high=100)
       n_events = generator.integers(low=200, high=2000)

       fake = Faker()
       users = [fake.ipv4() for _ in range(n_users)]

       return pd.DataFrame(
           {
               "user": generator.choice(users, size=n_events, replace=True),
               "date": date.strftime("%Y-%m-%d"),
           }
       )

   app = Flask(__name__)
   app.config["events"] = _generate_events(end_date=date(year=2019, month=1, day=5))

   @app.route("/events")
   def events():
       start_date = _str_to_datetime(request.args.get("start_date", None))
       end_date = _str_to_datetime(request.args.get("end_date", None))

       events = app.config.get("events")
       if start_date is not None:
           events = events.loc[events["date"] >= start_date]
       if end_date is not None:
           events = events.loc[events["date"] < end_date]

       return jsonify(events.to_dict(orient="records"))

   def _str_to_datetime(value):
       if value is None:
           return None
       return datetime.strptime(value, "%Y-%m-%d")

   if __name__ == "__main__":
       app.run(host="0.0.0.0", port=5000)
   ```

   </Solution>

5. Build a DAG which calculates stats in a CSV file stored locally.

   <Solution>

   ```python fp=calculate_event_stats.py nu
   import datetime as dt
   from pathlib import Path

   import pandas as pd
   from airflow import DAG
   from airflow.operators.bash import BashOperator
   from airflow.operators.python import PythonOperator

   AIRFLOW_HOME = "/home/airflow"

   dag = DAG(
       dag_id="calculate_event_stats",
       start_date=dt.datetime(2019, 1, 1),
       end_date=dt.datetime(2019, 1, 5),
       schedule_interval=None,
   )

   fetch_events = BashOperator(
       task_id="fetch_events",
       bash_command=(
           f"mkdir -p {AIRFLOW_HOME}/data && "
           f"curl -o {AIRFLOW_HOME}/data/events.json http://events_api:5000/events"
       ),
       dag=dag,
   )

   def _calculate_stats(input_path, output_path):
       events = pd.read_json(input_path)
       stats = events.groupby(["date", "user"]).size().reset_index()
       Path(output_path).parent.mkdir(exist_ok=True)
       stats.to_csv(output_path, index=False)

   calculate_stats = PythonOperator(
       task_id="calculate_stats",
       python_callable=_calculate_stats,
       op_kwargs={
           "input_path": f"{AIRFLOW_HOME}/data/events.json",
           "output_path": f"{AIRFLOW_HOME}/data/stats.csv",
       },
       dag=dag,
   )

   fetch_events >> calculate_stats
   ```

   </Solution>

6. Write the cron expressions to schedule a run :

   - on the hour,
   - at midnight,
   - weekly,
   - midnight on the first of every month,
   - at 23:45 every Saturday,
   - every Monday, Wednesday, Friday at midnight,
   - every weekday at midnight,
   - every day at 00:00 and 12:00.

   <Solution>

   - `0 * * * *`
   - `0 0 * * *`
   - `0 0 * * 0`
   - `0 0 1 * *`
   - `45 23 * * SAT`
   - `0 0 * * MON,WED,FRI`
   - `0 0 * * MON-FRI`
   - `0 0,12 * * *`

   </Solution>

7. Which macro can be used to :

   - schedule once and only once,
   - run once an hour at the beginning of the hour,
   - run once a day at midnight,
   - run once a week at midnight on Sunday morning,
   - run once a month at midnight on the first day of the month,
   - run once a year at midnight on January 1.

   <Solution>

   - `@once`
   - `@hourly`
   - `@daily`
   - `@weekly`
   - `@monthly`
   - `@yearly`

   </Solution>

8. How to define a frequency-based schedule?

   <Solution>

   ```python
   dag = DAG(
       ...
       schedule_interval=dt.timedelta(days=3),
       # schedule_interval=dt.timedelta(minutes=10),
       # schedule_interval=dt.timedelta(hours=2),
   )
   ```

   </Solution>
